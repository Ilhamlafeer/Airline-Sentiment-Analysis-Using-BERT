## ğŸ¤– Airline Tweet Sentiment Classification Using BERT
This project fine-tunes a pretrained **BERT (bert-base-uncased)** model to classify airline-related tweets as **positive**, **neutral**, or **negative**. It improves upon traditional NLP techniques by applying **transformer-based transfer learning** for better accuracy and generalization.

### ğŸ“Œ Features
* ğŸ’¬ **Text Preprocessing**
  * Basic cleaning (e.g., lowercasing, URL removal)
  * Hugging Faceâ€™s `BertTokenizer` used for subword tokenization and input formatting

* ğŸ”§ **Model Fine-tuning**
  * Used Hugging Face `Trainer` API with `BertForSequenceClassification`
  * Fine-tuned on cleaned tweet dataset with 3 sentiment classes
  * Configured training with:
    * Learning rate: `2e-5`
    * Batch size: `16`
    * Epochs: `3`
    * Weight decay: `0.01`

* ğŸ“Š **Training Monitoring**
  * Integrated **Weights & Biases (wandb)** for experiment tracking and hyperparameter tuning

* ğŸ“ˆ **Evaluation**
  * Accuracy: **\~85.96%**
  * Evaluation Loss: **\~0.47**
  * Evaluation Speed: **\~146 samples/sec**

### ğŸ§  Techniques Used
* **Transfer Learning** with pretrained BERT
* **Tokenization** with `BertTokenizer`
* **Fine-tuning** on a real-world dataset using Hugging Face Transformers
* **Evaluation Metrics**: Accuracy, Cross-Entropy Loss
* **Experiment Tracking** with wandb
* Optimized model for limited hardware (Google Colab + T4 GPU)

### ğŸ“ Dataset
* **Source**: Kaggle - Airline Tweet Sentiment
* **Classes**: positive, neutral, negative
* **Text Format**: Short tweets mentioning airline services

### ğŸ§ª Results
* âœ… Final accuracy: **85.96%**
* âœ… Loss: **0.47**
* âœ… Model saved using `model.save_pretrained()` for future use or deployment

### ğŸ“¦ Requirements
* `transformers`
* `datasets`
* `pandas`
* `sklearn`
* `wandb`
* `nltk`
* `pytorch`
